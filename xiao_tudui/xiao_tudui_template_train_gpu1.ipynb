{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1: .cuda() \\\n",
    "&emsp; 1. Network Models \\\n",
    "&emsp; 2. Data \\\n",
    "&emsp; 3. Cost Function \\\n",
    "if cuda_is_available() \\\n",
    "\\\n",
    "Method 2: .to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Xinxie Wu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: 'Could not find module 'C:\\Users\\Xinxie Wu\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get training & testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\cifar-10-python.tar.gz to ../data\n"
     ]
    }
   ],
   "source": [
    "train_data = torchvision.datasets.CIFAR10(root='../data', train=True, transform=torchvision.transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "test_data = torchvision.datasets.CIFAR10(root='../data', train=False, transform=torchvision.transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Size: 50000\n",
      "Testing Dataset Size: 10000\n"
     ]
    }
   ],
   "source": [
    "train_data_size = len(train_data)\n",
    "test_data_size = len(test_data)\n",
    "print(f\"Training Dataset Size: {train_data_size}\")\n",
    "print(f\"Testing Dataset Size: {test_data_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use DataLoader to load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNetwork, self).__init__()\n",
    "        self.model1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 5, 1, padding=2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 32, 5, 1, padding=2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 5, 1, padding=2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024, 64),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model1(x)\n",
    "        return output\n",
    "\n",
    "mynetwork = MyNetwork()\n",
    "# Model to CUDA\n",
    "mynetwork.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Cost Function to CUDA\n",
    "loss_fn = loss_fn.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD(mynetwork.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up training networks' parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_training_step = 0\n",
    "total_test_step = 0\n",
    "epoch = 10\n",
    "writer = SummaryWriter(\"../logs_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training - Accuracy: argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------epoch: 11------------\n",
      "Traning: 1; Loss: 2.2799246311187744\n",
      "Traning: 2; Loss: 2.2992379665374756\n",
      "Traning: 3; Loss: 2.316944122314453\n",
      "Traning: 4; Loss: 2.3101553916931152\n",
      "Traning: 5; Loss: 2.293660879135132\n",
      "Traning: 6; Loss: 2.30493426322937\n",
      "Traning: 7; Loss: 2.3003199100494385\n",
      "Traning: 8; Loss: 2.30916690826416\n",
      "Traning: 9; Loss: 2.310387372970581\n",
      "Traning: 10; Loss: 2.305821418762207\n",
      "Traning: 11; Loss: 2.3012149333953857\n",
      "Traning: 12; Loss: 2.3175010681152344\n",
      "Traning: 13; Loss: 2.2893807888031006\n",
      "Traning: 14; Loss: 2.296213150024414\n",
      "Traning: 15; Loss: 2.315534830093384\n",
      "Traning: 16; Loss: 2.3032915592193604\n",
      "Traning: 17; Loss: 2.305190324783325\n",
      "Traning: 18; Loss: 2.3016738891601562\n",
      "Traning: 19; Loss: 2.3112761974334717\n",
      "Traning: 20; Loss: 2.3090178966522217\n",
      "Traning: 21; Loss: 2.3039979934692383\n",
      "Traning: 22; Loss: 2.3153862953186035\n",
      "Traning: 23; Loss: 2.2937655448913574\n",
      "Traning: 24; Loss: 2.2984673976898193\n",
      "Traning: 25; Loss: 2.3024356365203857\n",
      "Traning: 26; Loss: 2.3078083992004395\n",
      "Traning: 27; Loss: 2.295377254486084\n",
      "Traning: 28; Loss: 2.3042821884155273\n",
      "Traning: 29; Loss: 2.3099799156188965\n",
      "Traning: 30; Loss: 2.3174171447753906\n",
      "Traning: 31; Loss: 2.311927318572998\n",
      "Traning: 32; Loss: 2.296583652496338\n",
      "Traning: 33; Loss: 2.3071112632751465\n",
      "Traning: 34; Loss: 2.3018267154693604\n",
      "Traning: 35; Loss: 2.3006832599639893\n",
      "Traning: 36; Loss: 2.315593957901001\n",
      "Traning: 37; Loss: 2.294715166091919\n",
      "Traning: 38; Loss: 2.2989280223846436\n",
      "Traning: 39; Loss: 2.2991998195648193\n",
      "Traning: 40; Loss: 2.300583600997925\n",
      "Traning: 41; Loss: 2.29728102684021\n",
      "Traning: 42; Loss: 2.3008673191070557\n",
      "Traning: 43; Loss: 2.314387798309326\n",
      "Traning: 44; Loss: 2.2907681465148926\n",
      "Traning: 45; Loss: 2.2876579761505127\n",
      "Traning: 46; Loss: 2.309678316116333\n",
      "Traning: 47; Loss: 2.3042242527008057\n",
      "Traning: 48; Loss: 2.3068857192993164\n",
      "Traning: 49; Loss: 2.295511245727539\n",
      "Traning: 50; Loss: 2.3189220428466797\n",
      "Traning: 51; Loss: 2.29133939743042\n",
      "Traning: 52; Loss: 2.303439140319824\n",
      "Traning: 53; Loss: 2.2819387912750244\n",
      "Traning: 54; Loss: 2.3049094676971436\n",
      "Traning: 55; Loss: 2.300513505935669\n",
      "Traning: 56; Loss: 2.308248519897461\n",
      "Traning: 57; Loss: 2.29978609085083\n",
      "Traning: 58; Loss: 2.289254903793335\n",
      "Traning: 59; Loss: 2.308037042617798\n",
      "Traning: 60; Loss: 2.311246633529663\n",
      "Traning: 61; Loss: 2.2913100719451904\n",
      "Traning: 62; Loss: 2.3036248683929443\n",
      "Traning: 63; Loss: 2.3018639087677\n",
      "Traning: 64; Loss: 2.3004231452941895\n",
      "Traning: 65; Loss: 2.3050177097320557\n",
      "Traning: 66; Loss: 2.2963287830352783\n",
      "Traning: 67; Loss: 2.3066458702087402\n",
      "Traning: 68; Loss: 2.2969937324523926\n",
      "Traning: 69; Loss: 2.299664258956909\n",
      "Traning: 70; Loss: 2.3024260997772217\n",
      "Traning: 71; Loss: 2.2910053730010986\n",
      "Traning: 72; Loss: 2.286820650100708\n",
      "Traning: 73; Loss: 2.2892520427703857\n",
      "Traning: 74; Loss: 2.296696186065674\n",
      "Traning: 75; Loss: 2.298590898513794\n",
      "Traning: 76; Loss: 2.301706075668335\n",
      "Traning: 77; Loss: 2.3088786602020264\n",
      "Traning: 78; Loss: 2.2987465858459473\n",
      "Traning: 79; Loss: 2.300542116165161\n",
      "Traning: 80; Loss: 2.29657244682312\n",
      "Traning: 81; Loss: 2.297579050064087\n",
      "Traning: 82; Loss: 2.292743444442749\n",
      "Traning: 83; Loss: 2.300572156906128\n",
      "Traning: 84; Loss: 2.2919464111328125\n",
      "Traning: 85; Loss: 2.2944328784942627\n",
      "Traning: 86; Loss: 2.2946836948394775\n",
      "Traning: 87; Loss: 2.298316478729248\n",
      "Traning: 88; Loss: 2.2911269664764404\n",
      "Traning: 89; Loss: 2.2817935943603516\n",
      "Traning: 90; Loss: 2.298412322998047\n",
      "Traning: 91; Loss: 2.3068180084228516\n",
      "Traning: 92; Loss: 2.289774179458618\n",
      "Traning: 93; Loss: 2.297391176223755\n",
      "Traning: 94; Loss: 2.2806167602539062\n",
      "Traning: 95; Loss: 2.2999234199523926\n",
      "Traning: 96; Loss: 2.2916080951690674\n",
      "Traning: 97; Loss: 2.2997734546661377\n",
      "Traning: 98; Loss: 2.3051624298095703\n",
      "Traning: 99; Loss: 2.303213357925415\n",
      "Traning: 100; Loss: 2.2824273109436035\n",
      "Traning: 101; Loss: 2.302330493927002\n",
      "Traning: 102; Loss: 2.300809621810913\n",
      "Traning: 103; Loss: 2.305612087249756\n",
      "Traning: 104; Loss: 2.294700860977173\n",
      "Traning: 105; Loss: 2.290010929107666\n",
      "Traning: 106; Loss: 2.3074469566345215\n",
      "Traning: 107; Loss: 2.307509422302246\n",
      "Traning: 108; Loss: 2.3012099266052246\n",
      "Traning: 109; Loss: 2.2855682373046875\n",
      "Traning: 110; Loss: 2.3035836219787598\n",
      "Traning: 111; Loss: 2.3039722442626953\n",
      "Traning: 112; Loss: 2.3030552864074707\n",
      "Traning: 113; Loss: 2.2961173057556152\n",
      "Traning: 114; Loss: 2.298983573913574\n",
      "Traning: 115; Loss: 2.2927181720733643\n",
      "Traning: 116; Loss: 2.2984848022460938\n",
      "Traning: 117; Loss: 2.2841999530792236\n",
      "Traning: 118; Loss: 2.2770466804504395\n",
      "Traning: 119; Loss: 2.298321485519409\n",
      "Traning: 120; Loss: 2.2939083576202393\n",
      "Traning: 121; Loss: 2.2877848148345947\n",
      "Traning: 122; Loss: 2.2848854064941406\n",
      "Traning: 123; Loss: 2.302501678466797\n",
      "Traning: 124; Loss: 2.289691209793091\n",
      "Traning: 125; Loss: 2.2901296615600586\n",
      "Traning: 126; Loss: 2.2873661518096924\n",
      "Traning: 127; Loss: 2.273312568664551\n",
      "Traning: 128; Loss: 2.28201961517334\n",
      "Traning: 129; Loss: 2.2918481826782227\n",
      "Traning: 130; Loss: 2.2882890701293945\n",
      "Traning: 131; Loss: 2.293236255645752\n",
      "Traning: 132; Loss: 2.2872209548950195\n",
      "Traning: 133; Loss: 2.3101015090942383\n",
      "Traning: 134; Loss: 2.273756742477417\n",
      "Traning: 135; Loss: 2.3023180961608887\n",
      "Traning: 136; Loss: 2.2853095531463623\n",
      "Traning: 137; Loss: 2.2961127758026123\n",
      "Traning: 138; Loss: 2.2890467643737793\n",
      "Traning: 139; Loss: 2.278388023376465\n",
      "Traning: 140; Loss: 2.305687427520752\n",
      "Traning: 141; Loss: 2.2970073223114014\n",
      "Traning: 142; Loss: 2.2804372310638428\n",
      "Traning: 143; Loss: 2.283133029937744\n",
      "Traning: 144; Loss: 2.291386127471924\n",
      "Traning: 145; Loss: 2.294705629348755\n",
      "Traning: 146; Loss: 2.283811569213867\n",
      "Traning: 147; Loss: 2.286829710006714\n",
      "Traning: 148; Loss: 2.282505512237549\n",
      "Traning: 149; Loss: 2.278860092163086\n",
      "Traning: 150; Loss: 2.2919766902923584\n",
      "Traning: 151; Loss: 2.286862373352051\n",
      "Traning: 152; Loss: 2.2806122303009033\n",
      "Traning: 153; Loss: 2.311321258544922\n",
      "Traning: 154; Loss: 2.2958807945251465\n",
      "Traning: 155; Loss: 2.275373935699463\n",
      "Traning: 156; Loss: 2.295013904571533\n",
      "Traning: 157; Loss: 2.2871196269989014\n",
      "Traning: 158; Loss: 2.2940733432769775\n",
      "Traning: 159; Loss: 2.27939510345459\n",
      "Traning: 160; Loss: 2.3080718517303467\n",
      "Traning: 161; Loss: 2.2730801105499268\n",
      "Traning: 162; Loss: 2.271054267883301\n",
      "Traning: 163; Loss: 2.2959961891174316\n",
      "Traning: 164; Loss: 2.293198347091675\n",
      "Traning: 165; Loss: 2.2965128421783447\n",
      "Traning: 166; Loss: 2.2845864295959473\n",
      "Traning: 167; Loss: 2.2663238048553467\n",
      "Traning: 168; Loss: 2.300048828125\n",
      "Traning: 169; Loss: 2.273784875869751\n",
      "Traning: 170; Loss: 2.2869083881378174\n",
      "Traning: 171; Loss: 2.2938148975372314\n",
      "Traning: 172; Loss: 2.277555465698242\n",
      "Traning: 173; Loss: 2.285588502883911\n",
      "Traning: 174; Loss: 2.2901952266693115\n",
      "Traning: 175; Loss: 2.2956371307373047\n",
      "Traning: 176; Loss: 2.26951265335083\n",
      "Traning: 177; Loss: 2.2716569900512695\n",
      "Traning: 178; Loss: 2.2996537685394287\n",
      "Traning: 179; Loss: 2.296670913696289\n",
      "Traning: 180; Loss: 2.294715404510498\n",
      "Traning: 181; Loss: 2.2854526042938232\n",
      "Traning: 182; Loss: 2.2888500690460205\n",
      "Traning: 183; Loss: 2.2690317630767822\n",
      "Traning: 184; Loss: 2.289742946624756\n",
      "Traning: 185; Loss: 2.2955987453460693\n",
      "Traning: 186; Loss: 2.3009090423583984\n",
      "Traning: 187; Loss: 2.28214430809021\n",
      "Traning: 188; Loss: 2.2846603393554688\n",
      "Traning: 189; Loss: 2.270366907119751\n",
      "Traning: 190; Loss: 2.2827558517456055\n",
      "Traning: 191; Loss: 2.2795212268829346\n",
      "Traning: 192; Loss: 2.2685766220092773\n",
      "Traning: 193; Loss: 2.2878146171569824\n",
      "Traning: 194; Loss: 2.2803690433502197\n",
      "Traning: 195; Loss: 2.271697998046875\n",
      "Traning: 196; Loss: 2.2780444622039795\n",
      "Traning: 197; Loss: 2.2960970401763916\n",
      "Traning: 198; Loss: 2.2793307304382324\n",
      "Traning: 199; Loss: 2.2972960472106934\n",
      "Traning: 200; Loss: 2.2814998626708984\n",
      "Traning: 201; Loss: 2.2760584354400635\n",
      "Traning: 202; Loss: 2.2727129459381104\n",
      "Traning: 203; Loss: 2.2922515869140625\n",
      "Traning: 204; Loss: 2.276871919631958\n",
      "Traning: 205; Loss: 2.2627711296081543\n",
      "Traning: 206; Loss: 2.3040659427642822\n",
      "Traning: 207; Loss: 2.2815909385681152\n",
      "Traning: 208; Loss: 2.294459104537964\n",
      "Traning: 209; Loss: 2.274547576904297\n",
      "Traning: 210; Loss: 2.2922399044036865\n",
      "Traning: 211; Loss: 2.2709476947784424\n",
      "Traning: 212; Loss: 2.2817890644073486\n",
      "Traning: 213; Loss: 2.2561914920806885\n",
      "Traning: 214; Loss: 2.2764978408813477\n",
      "Traning: 215; Loss: 2.272578239440918\n",
      "Traning: 216; Loss: 2.291945695877075\n",
      "Traning: 217; Loss: 2.2753138542175293\n",
      "Traning: 218; Loss: 2.29685115814209\n",
      "Traning: 219; Loss: 2.2917239665985107\n",
      "Traning: 220; Loss: 2.2715392112731934\n",
      "Traning: 221; Loss: 2.2949466705322266\n",
      "Traning: 222; Loss: 2.2716245651245117\n",
      "Traning: 223; Loss: 2.3000760078430176\n",
      "Traning: 224; Loss: 2.2765421867370605\n",
      "Traning: 225; Loss: 2.287846803665161\n",
      "Traning: 226; Loss: 2.2761435508728027\n",
      "Traning: 227; Loss: 2.2810871601104736\n",
      "Traning: 228; Loss: 2.2641568183898926\n",
      "Traning: 229; Loss: 2.279407501220703\n",
      "Traning: 230; Loss: 2.2926688194274902\n",
      "Traning: 231; Loss: 2.276123523712158\n",
      "Traning: 232; Loss: 2.2673683166503906\n",
      "Traning: 233; Loss: 2.2769744396209717\n",
      "Traning: 234; Loss: 2.3014822006225586\n",
      "Traning: 235; Loss: 2.292597532272339\n",
      "Traning: 236; Loss: 2.2751026153564453\n",
      "Traning: 237; Loss: 2.2775533199310303\n",
      "Traning: 238; Loss: 2.2840192317962646\n",
      "Traning: 239; Loss: 2.27422833442688\n",
      "Traning: 240; Loss: 2.2759385108947754\n",
      "Traning: 241; Loss: 2.2982897758483887\n",
      "Traning: 242; Loss: 2.291367292404175\n",
      "Traning: 243; Loss: 2.2698464393615723\n",
      "Traning: 244; Loss: 2.279367685317993\n",
      "Traning: 245; Loss: 2.2661056518554688\n",
      "Traning: 246; Loss: 2.2844960689544678\n",
      "Traning: 247; Loss: 2.288006067276001\n",
      "Traning: 248; Loss: 2.267643928527832\n",
      "Traning: 249; Loss: 2.268173933029175\n",
      "Traning: 250; Loss: 2.283344268798828\n",
      "Traning: 251; Loss: 2.2669472694396973\n",
      "Traning: 252; Loss: 2.254091739654541\n",
      "Traning: 253; Loss: 2.261181354522705\n",
      "Traning: 254; Loss: 2.2716431617736816\n",
      "Traning: 255; Loss: 2.2664318084716797\n",
      "Traning: 256; Loss: 2.278503656387329\n",
      "Traning: 257; Loss: 2.271759271621704\n",
      "Traning: 258; Loss: 2.2637155055999756\n",
      "Traning: 259; Loss: 2.2616453170776367\n",
      "Traning: 260; Loss: 2.2750391960144043\n",
      "Traning: 261; Loss: 2.270545721054077\n",
      "Traning: 262; Loss: 2.256709337234497\n",
      "Traning: 263; Loss: 2.281825542449951\n",
      "Traning: 264; Loss: 2.296407461166382\n",
      "Traning: 265; Loss: 2.2788283824920654\n",
      "Traning: 266; Loss: 2.2546486854553223\n",
      "Traning: 267; Loss: 2.279313087463379\n",
      "Traning: 268; Loss: 2.2781035900115967\n",
      "Traning: 269; Loss: 2.262955904006958\n",
      "Traning: 270; Loss: 2.2455813884735107\n",
      "Traning: 271; Loss: 2.2913968563079834\n",
      "Traning: 272; Loss: 2.2601318359375\n",
      "Traning: 273; Loss: 2.2576847076416016\n",
      "Traning: 274; Loss: 2.268129587173462\n",
      "Traning: 275; Loss: 2.299992084503174\n",
      "Traning: 276; Loss: 2.2869980335235596\n",
      "Traning: 277; Loss: 2.289090394973755\n",
      "Traning: 278; Loss: 2.260775327682495\n",
      "Traning: 279; Loss: 2.274726629257202\n",
      "Traning: 280; Loss: 2.260603427886963\n",
      "Traning: 281; Loss: 2.242408514022827\n",
      "Traning: 282; Loss: 2.278275489807129\n",
      "Traning: 283; Loss: 2.267014503479004\n",
      "Traning: 284; Loss: 2.281510591506958\n",
      "Traning: 285; Loss: 2.247544527053833\n",
      "Traning: 286; Loss: 2.272447109222412\n",
      "Traning: 287; Loss: 2.2788636684417725\n",
      "Traning: 288; Loss: 2.2568085193634033\n",
      "Traning: 289; Loss: 2.2776646614074707\n",
      "Traning: 290; Loss: 2.246764898300171\n",
      "Traning: 291; Loss: 2.255962371826172\n",
      "Traning: 292; Loss: 2.2689573764801025\n",
      "Traning: 293; Loss: 2.2461419105529785\n",
      "Traning: 294; Loss: 2.2385787963867188\n",
      "Traning: 295; Loss: 2.266617774963379\n",
      "Traning: 296; Loss: 2.2597737312316895\n",
      "Traning: 297; Loss: 2.275921583175659\n",
      "Traning: 298; Loss: 2.25166392326355\n",
      "Traning: 299; Loss: 2.223578929901123\n",
      "Traning: 300; Loss: 2.258758783340454\n",
      "Traning: 301; Loss: 2.277168035507202\n",
      "Traning: 302; Loss: 2.260895252227783\n",
      "Traning: 303; Loss: 2.251568555831909\n",
      "Traning: 304; Loss: 2.26019549369812\n",
      "Traning: 305; Loss: 2.2721617221832275\n",
      "Traning: 306; Loss: 2.27419114112854\n",
      "Traning: 307; Loss: 2.2373247146606445\n",
      "Traning: 308; Loss: 2.243971586227417\n",
      "Traning: 309; Loss: 2.2343032360076904\n",
      "Traning: 310; Loss: 2.268859624862671\n",
      "Traning: 311; Loss: 2.258958339691162\n",
      "Traning: 312; Loss: 2.2671546936035156\n",
      "Traning: 313; Loss: 2.254997730255127\n",
      "Traning: 314; Loss: 2.2617132663726807\n",
      "Traning: 315; Loss: 2.2517621517181396\n",
      "Traning: 316; Loss: 2.272359848022461\n",
      "Traning: 317; Loss: 2.225371837615967\n",
      "Traning: 318; Loss: 2.276099681854248\n",
      "Traning: 319; Loss: 2.2173056602478027\n",
      "Traning: 320; Loss: 2.226743459701538\n",
      "Traning: 321; Loss: 2.250936508178711\n",
      "Traning: 322; Loss: 2.2718207836151123\n",
      "Traning: 323; Loss: 2.2431931495666504\n",
      "Traning: 324; Loss: 2.2051093578338623\n",
      "Traning: 325; Loss: 2.2645936012268066\n",
      "Traning: 326; Loss: 2.229079484939575\n",
      "Traning: 327; Loss: 2.259007215499878\n",
      "Traning: 328; Loss: 2.2269368171691895\n",
      "Traning: 329; Loss: 2.2535974979400635\n",
      "Traning: 330; Loss: 2.2666683197021484\n",
      "Traning: 331; Loss: 2.2302300930023193\n",
      "Traning: 332; Loss: 2.2447350025177\n",
      "Traning: 333; Loss: 2.1999738216400146\n",
      "Traning: 334; Loss: 2.26983642578125\n",
      "Traning: 335; Loss: 2.2166271209716797\n",
      "Traning: 336; Loss: 2.2074406147003174\n",
      "Traning: 337; Loss: 2.2364771366119385\n",
      "Traning: 338; Loss: 2.267716884613037\n",
      "Traning: 339; Loss: 2.2370216846466064\n",
      "Traning: 340; Loss: 2.209547996520996\n",
      "Traning: 341; Loss: 2.2250003814697266\n",
      "Traning: 342; Loss: 2.2189435958862305\n",
      "Traning: 343; Loss: 2.2597126960754395\n",
      "Traning: 344; Loss: 2.2660813331604004\n",
      "Traning: 345; Loss: 2.2448480129241943\n",
      "Traning: 346; Loss: 2.2320783138275146\n",
      "Traning: 347; Loss: 2.2273519039154053\n",
      "Traning: 348; Loss: 2.2508444786071777\n",
      "Traning: 349; Loss: 2.250488758087158\n",
      "Traning: 350; Loss: 2.2509241104125977\n",
      "Traning: 351; Loss: 2.2168502807617188\n",
      "Traning: 352; Loss: 2.2403430938720703\n",
      "Traning: 353; Loss: 2.2300546169281006\n",
      "Traning: 354; Loss: 2.228379011154175\n",
      "Traning: 355; Loss: 2.1839592456817627\n",
      "Traning: 356; Loss: 2.2687489986419678\n",
      "Traning: 357; Loss: 2.2262163162231445\n",
      "Traning: 358; Loss: 2.2318897247314453\n",
      "Traning: 359; Loss: 2.2230470180511475\n",
      "Traning: 360; Loss: 2.2340168952941895\n",
      "Traning: 361; Loss: 2.2545018196105957\n",
      "Traning: 362; Loss: 2.2125117778778076\n",
      "Traning: 363; Loss: 2.214341878890991\n",
      "Traning: 364; Loss: 2.222269058227539\n",
      "Traning: 365; Loss: 2.1992897987365723\n",
      "Traning: 366; Loss: 2.203434467315674\n",
      "Traning: 367; Loss: 2.2060508728027344\n",
      "Traning: 368; Loss: 2.263193368911743\n",
      "Traning: 369; Loss: 2.19846510887146\n",
      "Traning: 370; Loss: 2.1823925971984863\n",
      "Traning: 371; Loss: 2.189357042312622\n",
      "Traning: 372; Loss: 2.1929972171783447\n",
      "Traning: 373; Loss: 2.151484966278076\n",
      "Traning: 374; Loss: 2.234809160232544\n",
      "Traning: 375; Loss: 2.2329468727111816\n",
      "Traning: 376; Loss: 2.2220046520233154\n",
      "Traning: 377; Loss: 2.176361560821533\n",
      "Traning: 378; Loss: 2.1609225273132324\n",
      "Traning: 379; Loss: 2.259892225265503\n",
      "Traning: 380; Loss: 2.175938129425049\n",
      "Traning: 381; Loss: 2.2044525146484375\n",
      "Traning: 382; Loss: 2.1989119052886963\n",
      "Traning: 383; Loss: 2.169180154800415\n",
      "Traning: 384; Loss: 2.200024366378784\n",
      "Traning: 385; Loss: 2.191551446914673\n",
      "Traning: 386; Loss: 2.221893072128296\n",
      "Traning: 387; Loss: 2.1240828037261963\n",
      "Traning: 388; Loss: 2.2816290855407715\n",
      "Traning: 389; Loss: 2.2051641941070557\n",
      "Traning: 390; Loss: 2.2054331302642822\n",
      "Traning: 391; Loss: 2.2069742679595947\n",
      "Traning: 392; Loss: 2.212224006652832\n",
      "Traning: 393; Loss: 2.184623956680298\n",
      "Traning: 394; Loss: 2.178008556365967\n",
      "Traning: 395; Loss: 2.1498754024505615\n",
      "Traning: 396; Loss: 2.2017018795013428\n",
      "Traning: 397; Loss: 2.2248075008392334\n",
      "Traning: 398; Loss: 2.191810369491577\n",
      "Traning: 399; Loss: 2.15681529045105\n",
      "Traning: 400; Loss: 2.1831047534942627\n",
      "Traning: 401; Loss: 2.1844165325164795\n",
      "Traning: 402; Loss: 2.2088043689727783\n",
      "Traning: 403; Loss: 2.168311834335327\n",
      "Traning: 404; Loss: 2.1418159008026123\n",
      "Traning: 405; Loss: 2.2500314712524414\n",
      "Traning: 406; Loss: 2.1874287128448486\n",
      "Traning: 407; Loss: 2.122490882873535\n",
      "Traning: 408; Loss: 2.1501972675323486\n",
      "Traning: 409; Loss: 2.154498338699341\n",
      "Traning: 410; Loss: 2.136343240737915\n",
      "Traning: 411; Loss: 2.238060474395752\n",
      "Traning: 412; Loss: 2.1685950756073\n",
      "Traning: 413; Loss: 2.206216335296631\n",
      "Traning: 414; Loss: 2.196526288986206\n",
      "Traning: 415; Loss: 2.1832897663116455\n",
      "Traning: 416; Loss: 2.12544322013855\n",
      "Traning: 417; Loss: 2.1504788398742676\n",
      "Traning: 418; Loss: 2.1752755641937256\n",
      "Traning: 419; Loss: 2.1568262577056885\n",
      "Traning: 420; Loss: 2.176314115524292\n",
      "Traning: 421; Loss: 2.231250524520874\n",
      "Traning: 422; Loss: 2.1515583992004395\n",
      "Traning: 423; Loss: 2.144416570663452\n",
      "Traning: 424; Loss: 2.1768651008605957\n",
      "Traning: 425; Loss: 2.1768343448638916\n",
      "Traning: 426; Loss: 2.202305555343628\n",
      "Traning: 427; Loss: 2.1931347846984863\n",
      "Traning: 428; Loss: 2.127810478210449\n",
      "Traning: 429; Loss: 2.164339780807495\n",
      "Traning: 430; Loss: 2.194284200668335\n",
      "Traning: 431; Loss: 2.152097463607788\n",
      "Traning: 432; Loss: 2.130868434906006\n",
      "Traning: 433; Loss: 2.1912641525268555\n",
      "Traning: 434; Loss: 2.1687872409820557\n",
      "Traning: 435; Loss: 2.156930685043335\n",
      "Traning: 436; Loss: 2.1173899173736572\n",
      "Traning: 437; Loss: 2.108102321624756\n",
      "Traning: 438; Loss: 2.1401705741882324\n",
      "Traning: 439; Loss: 2.1530497074127197\n",
      "Traning: 440; Loss: 2.2452590465545654\n",
      "Traning: 441; Loss: 2.202901840209961\n",
      "Traning: 442; Loss: 2.1508586406707764\n",
      "Traning: 443; Loss: 2.136078357696533\n",
      "Traning: 444; Loss: 2.115546703338623\n",
      "Traning: 445; Loss: 2.1535372734069824\n",
      "Traning: 446; Loss: 2.134380340576172\n",
      "Traning: 447; Loss: 2.14990234375\n",
      "Traning: 448; Loss: 2.1693718433380127\n",
      "Traning: 449; Loss: 2.090728521347046\n",
      "Traning: 450; Loss: 2.145456075668335\n",
      "Traning: 451; Loss: 2.150717258453369\n",
      "Traning: 452; Loss: 2.056882858276367\n",
      "Traning: 453; Loss: 2.166475772857666\n",
      "Traning: 454; Loss: 2.1950576305389404\n",
      "Traning: 455; Loss: 2.193932056427002\n",
      "Traning: 456; Loss: 2.0727224349975586\n",
      "Traning: 457; Loss: 2.132035732269287\n",
      "Traning: 458; Loss: 2.156313896179199\n",
      "Traning: 459; Loss: 2.1385769844055176\n",
      "Traning: 460; Loss: 2.091221570968628\n",
      "Traning: 461; Loss: 2.176140785217285\n",
      "Traning: 462; Loss: 2.130441427230835\n",
      "Traning: 463; Loss: 2.104550838470459\n",
      "Traning: 464; Loss: 2.1440672874450684\n",
      "Traning: 465; Loss: 2.064952850341797\n",
      "Traning: 466; Loss: 2.137627601623535\n",
      "Traning: 467; Loss: 2.1207127571105957\n",
      "Traning: 468; Loss: 2.0080251693725586\n",
      "Traning: 469; Loss: 2.1106956005096436\n",
      "Traning: 470; Loss: 2.070986270904541\n",
      "Traning: 471; Loss: 2.0666985511779785\n",
      "Traning: 472; Loss: 2.1579203605651855\n",
      "Traning: 473; Loss: 2.0795116424560547\n",
      "Traning: 474; Loss: 2.0700459480285645\n",
      "Traning: 475; Loss: 2.0482826232910156\n",
      "Traning: 476; Loss: 2.1119160652160645\n",
      "Traning: 477; Loss: 2.115243673324585\n",
      "Traning: 478; Loss: 2.0857231616973877\n",
      "Traning: 479; Loss: 2.154287338256836\n",
      "Traning: 480; Loss: 2.0868144035339355\n",
      "Traning: 481; Loss: 2.1187520027160645\n",
      "Traning: 482; Loss: 2.1740307807922363\n",
      "Traning: 483; Loss: 2.078946590423584\n",
      "Traning: 484; Loss: 2.0889382362365723\n",
      "Traning: 485; Loss: 2.0686392784118652\n",
      "Traning: 486; Loss: 2.051381826400757\n",
      "Traning: 487; Loss: 2.187579870223999\n",
      "Traning: 488; Loss: 2.089873790740967\n",
      "Traning: 489; Loss: 2.0925161838531494\n",
      "Traning: 490; Loss: 2.1067416667938232\n",
      "Traning: 491; Loss: 2.144212245941162\n",
      "Traning: 492; Loss: 2.1162214279174805\n",
      "Traning: 493; Loss: 2.111644744873047\n",
      "Traning: 494; Loss: 2.0701444149017334\n",
      "Traning: 495; Loss: 2.099696636199951\n",
      "Traning: 496; Loss: 2.044039726257324\n",
      "Traning: 497; Loss: 2.0920157432556152\n",
      "Traning: 498; Loss: 2.068192958831787\n",
      "Traning: 499; Loss: 2.100132465362549\n",
      "Traning: 500; Loss: 2.0556540489196777\n",
      "Traning: 501; Loss: 2.094594717025757\n",
      "Traning: 502; Loss: 2.0431337356567383\n",
      "Traning: 503; Loss: 2.013596773147583\n",
      "Traning: 504; Loss: 2.15468168258667\n",
      "Traning: 505; Loss: 2.0297327041625977\n",
      "Traning: 506; Loss: 1.9594045877456665\n",
      "Traning: 507; Loss: 2.018655776977539\n",
      "Traning: 508; Loss: 2.0894312858581543\n",
      "Traning: 509; Loss: 2.254755735397339\n",
      "Traning: 510; Loss: 2.32122802734375\n",
      "Traning: 511; Loss: 2.1398892402648926\n",
      "Traning: 512; Loss: 2.1259255409240723\n",
      "Traning: 513; Loss: 2.1896121501922607\n",
      "Traning: 514; Loss: 2.107985258102417\n",
      "Traning: 515; Loss: 2.237438917160034\n",
      "Traning: 516; Loss: 2.075488805770874\n",
      "Traning: 517; Loss: 2.1584627628326416\n",
      "Traning: 518; Loss: 1.9930393695831299\n",
      "Traning: 519; Loss: 1.9997214078903198\n",
      "Traning: 520; Loss: 2.0919013023376465\n",
      "Traning: 521; Loss: 2.131821870803833\n",
      "Traning: 522; Loss: 2.119760274887085\n",
      "Traning: 523; Loss: 2.0640370845794678\n",
      "Traning: 524; Loss: 2.082206964492798\n",
      "Traning: 525; Loss: 2.0456531047821045\n",
      "Traning: 526; Loss: 2.135529041290283\n",
      "Traning: 527; Loss: 2.0714709758758545\n",
      "Traning: 528; Loss: 2.047532320022583\n",
      "Traning: 529; Loss: 2.0952401161193848\n",
      "Traning: 530; Loss: 2.16059947013855\n",
      "Traning: 531; Loss: 2.069291591644287\n",
      "Traning: 532; Loss: 2.108619213104248\n",
      "Traning: 533; Loss: 2.1164755821228027\n",
      "Traning: 534; Loss: 1.953139305114746\n",
      "Traning: 535; Loss: 2.0214786529541016\n",
      "Traning: 536; Loss: 2.056119203567505\n",
      "Traning: 537; Loss: 2.048811197280884\n",
      "Traning: 538; Loss: 1.9792532920837402\n",
      "Traning: 539; Loss: 2.0779972076416016\n",
      "Traning: 540; Loss: 2.0007944107055664\n",
      "Traning: 541; Loss: 2.0137267112731934\n",
      "Traning: 542; Loss: 2.0463945865631104\n",
      "Traning: 543; Loss: 2.0104119777679443\n",
      "Traning: 544; Loss: 1.973329782485962\n",
      "Traning: 545; Loss: 2.0033509731292725\n",
      "Traning: 546; Loss: 2.0224382877349854\n",
      "Traning: 547; Loss: 2.111419439315796\n",
      "Traning: 548; Loss: 2.1053476333618164\n",
      "Traning: 549; Loss: 2.094480037689209\n",
      "Traning: 550; Loss: 2.111828327178955\n",
      "Traning: 551; Loss: 2.121964931488037\n",
      "Traning: 552; Loss: 2.2248568534851074\n",
      "Traning: 553; Loss: 2.0308942794799805\n",
      "Traning: 554; Loss: 2.1115033626556396\n",
      "Traning: 555; Loss: 2.034574031829834\n",
      "Traning: 556; Loss: 1.9576306343078613\n",
      "Traning: 557; Loss: 2.1552865505218506\n",
      "Traning: 558; Loss: 2.1076245307922363\n",
      "Traning: 559; Loss: 2.1801016330718994\n",
      "Traning: 560; Loss: 2.0160608291625977\n",
      "Traning: 561; Loss: 1.9352750778198242\n",
      "Traning: 562; Loss: 2.047797441482544\n",
      "Traning: 563; Loss: 2.1000726222991943\n",
      "Traning: 564; Loss: 2.208803415298462\n",
      "Traning: 565; Loss: 2.0475075244903564\n",
      "Traning: 566; Loss: 1.9796931743621826\n",
      "Traning: 567; Loss: 2.0152342319488525\n",
      "Traning: 568; Loss: 2.0094079971313477\n",
      "Traning: 569; Loss: 1.9580917358398438\n",
      "Traning: 570; Loss: 2.086893081665039\n",
      "Traning: 571; Loss: 2.1134347915649414\n",
      "Traning: 572; Loss: 2.0430026054382324\n",
      "Traning: 573; Loss: 2.0223827362060547\n",
      "Traning: 574; Loss: 2.0425331592559814\n",
      "Traning: 575; Loss: 1.9763751029968262\n",
      "Traning: 576; Loss: 2.157910108566284\n",
      "Traning: 577; Loss: 2.1153321266174316\n",
      "Traning: 578; Loss: 2.1398768424987793\n",
      "Traning: 579; Loss: 2.060337781906128\n",
      "Traning: 580; Loss: 2.0323398113250732\n",
      "Traning: 581; Loss: 2.0106894969940186\n",
      "Traning: 582; Loss: 1.983769416809082\n",
      "Traning: 583; Loss: 2.036097288131714\n",
      "Traning: 584; Loss: 2.102712392807007\n",
      "Traning: 585; Loss: 2.096583843231201\n",
      "Traning: 586; Loss: 2.0839691162109375\n",
      "Traning: 587; Loss: 2.064404010772705\n",
      "Traning: 588; Loss: 2.1176371574401855\n",
      "Traning: 589; Loss: 2.1212446689605713\n",
      "Traning: 590; Loss: 2.059607744216919\n",
      "Traning: 591; Loss: 2.1310689449310303\n",
      "Traning: 592; Loss: 1.9908833503723145\n",
      "Traning: 593; Loss: 1.9770362377166748\n",
      "Traning: 594; Loss: 2.188750982284546\n",
      "Traning: 595; Loss: 2.0543785095214844\n",
      "Traning: 596; Loss: 2.1171305179595947\n",
      "Traning: 597; Loss: 2.0200068950653076\n",
      "Traning: 598; Loss: 2.0191099643707275\n",
      "Traning: 599; Loss: 2.0362861156463623\n",
      "Traning: 600; Loss: 2.068060874938965\n",
      "Traning: 601; Loss: 2.176638603210449\n",
      "Traning: 602; Loss: 1.9537065029144287\n",
      "Traning: 603; Loss: 1.881420373916626\n",
      "Traning: 604; Loss: 2.0149781703948975\n",
      "Traning: 605; Loss: 2.101903200149536\n",
      "Traning: 606; Loss: 1.9434266090393066\n",
      "Traning: 607; Loss: 1.9572502374649048\n",
      "Traning: 608; Loss: 1.938912034034729\n",
      "Traning: 609; Loss: 1.9755667448043823\n",
      "Traning: 610; Loss: 1.9818214178085327\n",
      "Traning: 611; Loss: 1.9549989700317383\n",
      "Traning: 612; Loss: 2.1204099655151367\n",
      "Traning: 613; Loss: 2.0801444053649902\n",
      "Traning: 614; Loss: 2.081469774246216\n",
      "Traning: 615; Loss: 2.047177791595459\n",
      "Traning: 616; Loss: 2.045620918273926\n",
      "Traning: 617; Loss: 2.029574394226074\n",
      "Traning: 618; Loss: 2.044102907180786\n",
      "Traning: 619; Loss: 1.9723842144012451\n",
      "Traning: 620; Loss: 2.1311771869659424\n",
      "Traning: 621; Loss: 2.173693895339966\n",
      "Traning: 622; Loss: 2.213954210281372\n",
      "Traning: 623; Loss: 2.0302693843841553\n",
      "Traning: 624; Loss: 2.05181884765625\n",
      "Traning: 625; Loss: 2.1171607971191406\n",
      "Traning: 626; Loss: 1.9404417276382446\n",
      "Traning: 627; Loss: 1.8891212940216064\n",
      "Traning: 628; Loss: 2.1220641136169434\n",
      "Traning: 629; Loss: 2.164543867111206\n",
      "Traning: 630; Loss: 1.9770700931549072\n",
      "Traning: 631; Loss: 2.073561429977417\n",
      "Traning: 632; Loss: 2.0418403148651123\n",
      "Traning: 633; Loss: 1.9385008811950684\n",
      "Traning: 634; Loss: 1.9874067306518555\n",
      "Traning: 635; Loss: 2.042262554168701\n",
      "Traning: 636; Loss: 1.9050146341323853\n",
      "Traning: 637; Loss: 2.243223190307617\n",
      "Traning: 638; Loss: 2.024704694747925\n",
      "Traning: 639; Loss: 2.060835123062134\n",
      "Traning: 640; Loss: 2.1596627235412598\n",
      "Traning: 641; Loss: 2.1153206825256348\n",
      "Traning: 642; Loss: 2.1521525382995605\n",
      "Traning: 643; Loss: 2.0519819259643555\n",
      "Traning: 644; Loss: 2.0820772647857666\n",
      "Traning: 645; Loss: 1.954485535621643\n",
      "Traning: 646; Loss: 2.1431102752685547\n",
      "Traning: 647; Loss: 1.9736055135726929\n",
      "Traning: 648; Loss: 1.9816402196884155\n",
      "Traning: 649; Loss: 2.235830307006836\n",
      "Traning: 650; Loss: 2.0139005184173584\n",
      "Traning: 651; Loss: 1.9269516468048096\n",
      "Traning: 652; Loss: 2.0773398876190186\n",
      "Traning: 653; Loss: 1.8831820487976074\n",
      "Traning: 654; Loss: 1.9683854579925537\n",
      "Traning: 655; Loss: 1.9648714065551758\n",
      "Traning: 656; Loss: 2.082507848739624\n",
      "Traning: 657; Loss: 2.0385632514953613\n",
      "Traning: 658; Loss: 1.955574870109558\n",
      "Traning: 659; Loss: 2.030040740966797\n",
      "Traning: 660; Loss: 1.8716033697128296\n",
      "Traning: 661; Loss: 2.0031778812408447\n",
      "Traning: 662; Loss: 2.0766425132751465\n",
      "Traning: 663; Loss: 1.8883461952209473\n",
      "Traning: 664; Loss: 1.8767247200012207\n",
      "Traning: 665; Loss: 2.058422088623047\n",
      "Traning: 666; Loss: 2.0693726539611816\n",
      "Traning: 667; Loss: 2.0886127948760986\n",
      "Traning: 668; Loss: 2.2488861083984375\n",
      "Traning: 669; Loss: 1.9849289655685425\n",
      "Traning: 670; Loss: 2.0493505001068115\n",
      "Traning: 671; Loss: 2.0874781608581543\n",
      "Traning: 672; Loss: 1.9380208253860474\n",
      "Traning: 673; Loss: 1.8835989236831665\n",
      "Traning: 674; Loss: 2.001181125640869\n",
      "Traning: 675; Loss: 2.0773534774780273\n",
      "Traning: 676; Loss: 2.0256850719451904\n",
      "Traning: 677; Loss: 1.9852285385131836\n",
      "Traning: 678; Loss: 1.830191731452942\n",
      "Traning: 679; Loss: 2.0098824501037598\n",
      "Traning: 680; Loss: 2.0639708042144775\n",
      "Traning: 681; Loss: 2.0923690795898438\n",
      "Traning: 682; Loss: 1.9221135377883911\n",
      "Traning: 683; Loss: 1.962178111076355\n",
      "Traning: 684; Loss: 1.8995513916015625\n",
      "Traning: 685; Loss: 1.9854981899261475\n",
      "Traning: 686; Loss: 2.1577000617980957\n",
      "Traning: 687; Loss: 2.032458543777466\n",
      "Traning: 688; Loss: 1.8540782928466797\n",
      "Traning: 689; Loss: 1.9720516204833984\n",
      "Traning: 690; Loss: 1.9919930696487427\n",
      "Traning: 691; Loss: 1.9583027362823486\n",
      "Traning: 692; Loss: 2.009145975112915\n",
      "Traning: 693; Loss: 2.05583119392395\n",
      "Traning: 694; Loss: 2.1306018829345703\n",
      "Traning: 695; Loss: 2.077133893966675\n",
      "Traning: 696; Loss: 2.0570781230926514\n",
      "Traning: 697; Loss: 2.051647663116455\n",
      "Traning: 698; Loss: 2.051060914993286\n",
      "Traning: 699; Loss: 1.9726004600524902\n",
      "Traning: 700; Loss: 1.984079360961914\n",
      "Traning: 701; Loss: 1.9995574951171875\n",
      "Traning: 702; Loss: 2.158405303955078\n",
      "Traning: 703; Loss: 2.0027105808258057\n",
      "Traning: 704; Loss: 1.9366748332977295\n",
      "Traning: 705; Loss: 1.9154243469238281\n",
      "Traning: 706; Loss: 1.9531890153884888\n",
      "Traning: 707; Loss: 2.1335361003875732\n",
      "Traning: 708; Loss: 1.9740104675292969\n",
      "Traning: 709; Loss: 2.099533796310425\n",
      "Traning: 710; Loss: 1.994962215423584\n",
      "Traning: 711; Loss: 2.0987160205841064\n",
      "Traning: 712; Loss: 2.0053060054779053\n",
      "Traning: 713; Loss: 1.9303702116012573\n",
      "Traning: 714; Loss: 1.9920377731323242\n",
      "Traning: 715; Loss: 1.9094467163085938\n",
      "Traning: 716; Loss: 2.085503101348877\n",
      "Traning: 717; Loss: 1.9578325748443604\n",
      "Traning: 718; Loss: 1.9695292711257935\n",
      "Traning: 719; Loss: 1.8831706047058105\n",
      "Traning: 720; Loss: 1.994397759437561\n",
      "Traning: 721; Loss: 1.9213889837265015\n",
      "Traning: 722; Loss: 2.015618324279785\n",
      "Traning: 723; Loss: 1.9208533763885498\n",
      "Traning: 724; Loss: 2.0604336261749268\n",
      "Traning: 725; Loss: 2.0546226501464844\n",
      "Traning: 726; Loss: 1.8268256187438965\n",
      "Traning: 727; Loss: 2.08442759513855\n",
      "Traning: 728; Loss: 2.0402724742889404\n",
      "Traning: 729; Loss: 1.8505998849868774\n",
      "Traning: 730; Loss: 2.1153900623321533\n",
      "Traning: 731; Loss: 1.8708463907241821\n",
      "Traning: 732; Loss: 1.9778739213943481\n",
      "Traning: 733; Loss: 2.0861051082611084\n",
      "Traning: 734; Loss: 1.9473897218704224\n",
      "Traning: 735; Loss: 1.8898723125457764\n",
      "Traning: 736; Loss: 2.2143311500549316\n",
      "Traning: 737; Loss: 1.9943444728851318\n",
      "Traning: 738; Loss: 2.0532612800598145\n",
      "Traning: 739; Loss: 1.861862301826477\n",
      "Traning: 740; Loss: 1.9099684953689575\n",
      "Traning: 741; Loss: 1.9650977849960327\n",
      "Traning: 742; Loss: 1.9850565195083618\n",
      "Traning: 743; Loss: 2.083806037902832\n",
      "Traning: 744; Loss: 1.9174878597259521\n",
      "Traning: 745; Loss: 1.9119799137115479\n",
      "Traning: 746; Loss: 1.9871909618377686\n",
      "Traning: 747; Loss: 1.8383679389953613\n",
      "Traning: 748; Loss: 2.2780771255493164\n",
      "Traning: 749; Loss: 2.0894157886505127\n",
      "Traning: 750; Loss: 1.9447550773620605\n",
      "Traning: 751; Loss: 2.173780918121338\n",
      "Traning: 752; Loss: 2.1942038536071777\n",
      "Traning: 753; Loss: 1.949862003326416\n",
      "Traning: 754; Loss: 1.9502506256103516\n",
      "Traning: 755; Loss: 1.999644160270691\n",
      "Traning: 756; Loss: 1.8700546026229858\n",
      "Traning: 757; Loss: 2.0980281829833984\n",
      "Traning: 758; Loss: 1.85429048538208\n",
      "Traning: 759; Loss: 1.9191198348999023\n",
      "Traning: 760; Loss: 2.0194997787475586\n",
      "Traning: 761; Loss: 1.9768096208572388\n",
      "Traning: 762; Loss: 1.9850578308105469\n",
      "Traning: 763; Loss: 2.087195873260498\n",
      "Traning: 764; Loss: 1.986579179763794\n",
      "Traning: 765; Loss: 1.9164276123046875\n",
      "Traning: 766; Loss: 2.055509567260742\n",
      "Traning: 767; Loss: 2.0934712886810303\n",
      "Traning: 768; Loss: 2.044044256210327\n",
      "Traning: 769; Loss: 1.8775222301483154\n",
      "Traning: 770; Loss: 1.8764053583145142\n",
      "Traning: 771; Loss: 1.9936964511871338\n",
      "Traning: 772; Loss: 1.9091376066207886\n",
      "Traning: 773; Loss: 2.009592056274414\n",
      "Traning: 774; Loss: 2.0373013019561768\n",
      "Traning: 775; Loss: 2.057258129119873\n",
      "Traning: 776; Loss: 1.920286774635315\n",
      "Traning: 777; Loss: 2.0513858795166016\n",
      "Traning: 778; Loss: 1.9903171062469482\n",
      "Traning: 779; Loss: 2.088988780975342\n",
      "Traning: 780; Loss: 2.0182878971099854\n",
      "Traning: 781; Loss: 2.12499737739563\n",
      "Traning: 782; Loss: 2.1801490783691406\n",
      "------------epoch: 11------------\n",
      "Traning: 783; Loss: 2.0262107849121094\n",
      "Traning: 784; Loss: 1.9007115364074707\n",
      "Traning: 785; Loss: 2.000683307647705\n",
      "Traning: 786; Loss: 1.794236421585083\n",
      "Traning: 787; Loss: 1.8575464487075806\n",
      "Traning: 788; Loss: 1.914110541343689\n",
      "Traning: 789; Loss: 1.9077175855636597\n",
      "Traning: 790; Loss: 1.9239455461502075\n",
      "Traning: 791; Loss: 1.9174057245254517\n",
      "Traning: 792; Loss: 2.0633480548858643\n",
      "Traning: 793; Loss: 1.981064796447754\n",
      "Traning: 794; Loss: 1.8962348699569702\n",
      "Traning: 795; Loss: 2.0749027729034424\n",
      "Traning: 796; Loss: 2.052940845489502\n",
      "Traning: 797; Loss: 1.8980233669281006\n",
      "Traning: 798; Loss: 1.8753442764282227\n",
      "Traning: 799; Loss: 1.8790842294692993\n",
      "Traning: 800; Loss: 1.86995267868042\n",
      "Traning: 801; Loss: 1.9763866662979126\n",
      "Traning: 802; Loss: 1.8514701128005981\n",
      "Traning: 803; Loss: 1.9141358137130737\n",
      "Traning: 804; Loss: 1.8876187801361084\n",
      "Traning: 805; Loss: 1.8730981349945068\n",
      "Traning: 806; Loss: 1.9423613548278809\n",
      "Traning: 807; Loss: 1.9340604543685913\n",
      "Traning: 808; Loss: 1.869681715965271\n",
      "Traning: 809; Loss: 2.0085906982421875\n",
      "Traning: 810; Loss: 2.0842597484588623\n",
      "Traning: 811; Loss: 1.9344522953033447\n",
      "Traning: 812; Loss: 1.9655431509017944\n",
      "Traning: 813; Loss: 2.095284938812256\n",
      "Traning: 814; Loss: 2.2130050659179688\n",
      "Traning: 815; Loss: 2.177334785461426\n",
      "Traning: 816; Loss: 1.8200806379318237\n",
      "Traning: 817; Loss: 1.8947187662124634\n",
      "Traning: 818; Loss: 2.0764636993408203\n",
      "Traning: 819; Loss: 1.9547797441482544\n",
      "Traning: 820; Loss: 1.8127771615982056\n",
      "Traning: 821; Loss: 1.8532603979110718\n",
      "Traning: 822; Loss: 2.02528715133667\n",
      "Traning: 823; Loss: 2.0401384830474854\n",
      "Traning: 824; Loss: 1.954042911529541\n",
      "Traning: 825; Loss: 1.9461619853973389\n",
      "Traning: 826; Loss: 1.9155325889587402\n",
      "Traning: 827; Loss: 1.827877163887024\n",
      "Traning: 828; Loss: 1.9162219762802124\n",
      "Traning: 829; Loss: 2.0863473415374756\n",
      "Traning: 830; Loss: 1.840255618095398\n",
      "Traning: 831; Loss: 2.0270628929138184\n",
      "Traning: 832; Loss: 2.006873846054077\n",
      "Traning: 833; Loss: 1.9590345621109009\n",
      "Traning: 834; Loss: 1.9408717155456543\n",
      "Traning: 835; Loss: 1.8319286108016968\n",
      "Traning: 836; Loss: 2.113189697265625\n",
      "Traning: 837; Loss: 1.925459384918213\n",
      "Traning: 838; Loss: 1.9185116291046143\n",
      "Traning: 839; Loss: 1.9857217073440552\n",
      "Traning: 840; Loss: 2.0069801807403564\n",
      "Traning: 841; Loss: 1.8035647869110107\n",
      "Traning: 842; Loss: 1.8495804071426392\n",
      "Traning: 843; Loss: 2.004213571548462\n",
      "Traning: 844; Loss: 2.0751941204071045\n",
      "Traning: 845; Loss: 1.9223229885101318\n",
      "Traning: 846; Loss: 1.9372833967208862\n",
      "Traning: 847; Loss: 1.9317456483840942\n",
      "Traning: 848; Loss: 1.8466343879699707\n",
      "Traning: 849; Loss: 1.9181737899780273\n",
      "Traning: 850; Loss: 1.9789378643035889\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    print(f\"------------epoch: {i+1}------------\")\n",
    "\n",
    "    \"\"\"\n",
    "    Training\n",
    "    \"\"\"\n",
    "    mynetwork.train()\n",
    "    for data in train_dataloader:\n",
    "        imgs, targets = data\n",
    "        # Data to CUDA\n",
    "        imgs = imgs.cuda()\n",
    "        targets = targets.cuda()\n",
    "        outputs = mynetwork(imgs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_training_step += 1\n",
    "        if total_training_step % 100 == 0:\n",
    "            print(f\"Traning: {total_training_step}; Loss: {loss.item()}\")\n",
    "            writer.add_scalar(\"train_loss\", loss.item(), total_training_step)\n",
    "    \n",
    "    \"\"\"\n",
    "    Testing\n",
    "    \"\"\"\n",
    "    mynetwork.eval()\n",
    "    total_test_loss = 0\n",
    "    total_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_dataloader:\n",
    "            imgs, targets = data\n",
    "            # Data to CUDA\n",
    "            imgs = imgs.cuda()\n",
    "            targets = targets.cuda()\n",
    "            outputs = mynetwork(imgs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            total_test_loss += loss.item()\n",
    "            acc = (outputs.argmax(1) == targets).sum()\n",
    "            total_acc += acc\n",
    "    print(f\"total test loss: {total_test_loss}\")\n",
    "    print(f\"total accuracy: {total_acc/test_data_size}\")\n",
    "    writer.add_scalar(\"test_loss\", total_test_loss, total_test_step)\n",
    "    writer.add_scalar(\"test_accuracy\", total_acc/test_data_size, total_test_step)\n",
    "    total_test_step += 1\n",
    "\n",
    "    torch.save(mynetwork, f\"mynetwork_{i}.pth\")\n",
    "    # torch.save(mynetwork.state_dict(), f\"mynetwork_{i}.pth\")\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = torch.tensor([[0.1, 0.2],\n",
    "                        [0.05, 0.4]])\n",
    "print(outputs.argmax(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
